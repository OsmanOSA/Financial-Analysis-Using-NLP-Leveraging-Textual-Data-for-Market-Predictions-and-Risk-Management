{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5enyRdM5to8"
   },
   "source": [
    "# **Exploration de WordNet**\n",
    "\n",
    " \n",
    "\n",
    "Nous allons utiliser l'implementation NLTK de Wordnet comme premi√®re approche. La documentation est dispo ici: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "Charger WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TYL6Z2AH82F",
    "outputId": "cf91d9f8-c3bf-40f4-f1bd-4de3bfeac136"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z6Qy7hH2iEr"
   },
   "source": [
    "**Explorer les relations de synonymie**\n",
    "\n",
    "Essayons d'explorer les synonyms du mot \"motocar\". Ce dernier n'a qu'un seul sense, celui de \"car\", identifi√© par 'car.n.01', le premier sens du mot \"car. \n",
    "\n",
    "Synset('car.n.01') est appel√© synset ou ‚Äúsynonym set,‚Äù une collection/liste de mots *synonymes*. Ici n d√©signe un nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yqgicxmu2Z5X",
    "outputId": "4f485caf-9dc0-417f-ae94-a2a353892b88"
   },
   "outputs": [],
   "source": [
    "e=wn.synsets('motorcar')\n",
    "print(e)\n",
    "first_sense=e[0] # donne le premier sense du synset e\n",
    "first_sense.definition()\n",
    "dir(e[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqA5PAdbLKEp"
   },
   "source": [
    "A l'aide des m√©thodes accessibles √† partir de l'objet e[0] (i.e. dir(e[0]), retrouver:\n",
    "\n",
    "\n",
    "*   La d√©fintion du premier sense (sens1) du mot \"motocar\" \n",
    "*   La liste des mots (lemmes) synonymes du sens1 du mot \"motocar\"\n",
    "*   Des phrases exemples qui illustrent un des senses du mot \"motocar\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPJf_cUk83IA"
   },
   "outputs": [],
   "source": [
    "# r√©pondre aux questions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnYTiUgAOYmR"
   },
   "source": [
    "Le mot \"motocar\" a un seul sense. Essayons maintenant avec un mot polys√©mique, le mot \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qi52RKotOgfm",
    "outputId": "b14669f0-a426-462c-aaca-499b0d7ffe2d"
   },
   "outputs": [],
   "source": [
    "e=wn.synsets('car')\n",
    "print(e) #liste des senses du mot car\n",
    "i=1\n",
    "for s in e:\n",
    "  #print(s.relations())\n",
    "  #lemme=[i.lemmas() for i in e]\n",
    "  first_sense=s\n",
    "  print(\"sense \", i, s)\n",
    "  print(s.lemmas())\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gRNOZnRPKLg"
   },
   "source": [
    "**Explorer la hi√®rachie des relations ainsi que d'autres relations paradigmatiques**\n",
    "\n",
    "Explorer maintenant les relations d'hyperonymie et d'hyponymie des mots \"car\" et \"motocar\" (vous pouvez prendre d'autres mots). Vous pouvez explorer en parall√®le les r√©sultats sur la **d√©mo** en ligne (https://en-word.net/) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCz_HkA1PfQF",
    "outputId": "75f8f0b7-cb6d-46b6-e889-ee63af9d941a"
   },
   "outputs": [],
   "source": [
    "car_sense1=e[0]\n",
    "l=[i.lemmas() for i in car_sense1.hyponyms()]  #on explore ici les hyponymes du senses 1 du mot car\n",
    "print(l)\n",
    "\n",
    "# faire de m√™me avec l'hyperonym\n",
    "l=[i.lemmas() for i in car_sense1.hypernyms()]  #on explore ici les hyperonymes du senses 1 du mot car\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZBDbAAAQJsJ"
   },
   "source": [
    "Explorer les autres relations, comme la m√©ronymie, holonymie et l'antonymie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whYnHLhcQy5F"
   },
   "source": [
    "**Explorer la similarit√© s√©mantique entre les synsets**\n",
    "\n",
    "Etant donn√©s deux synsets, l'objectif est de parcourir les liens les reliants √† travers le r√©seau lexical.\n",
    "\n",
    "L'algorithme est le suivant : \n",
    "  * Each synset has one or more hypernym paths that link it to a root hypernym\n",
    "  * Two synsets linked to the same root may have several hypernyms\n",
    "in common. \n",
    "  * If two synsets share a very specific hypernym (one that\n",
    "is low down in the hypernym hierarchy) they must be closely related.\n",
    "\n",
    "Appliquons cette algorithme aux premiers senses des mots suivants (Vous pouvez aussi changer les mots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeQubgR8D0Az",
    "outputId": "5bd0b5d6-faa7-4308-cf6d-7e44a3d413c9"
   },
   "outputs": [],
   "source": [
    "mouse = wn.synsets('mouse')[0]\n",
    "cat = wn.synsets('cat')[0]\n",
    "dog = wn.synsets('dog')[0]\n",
    "\n",
    "dog.lowest_common_hypernyms(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMPnqdvQlemQ"
   },
   "source": [
    "Nous pouvons √©galement qualtifier la similarit√© entre synsets, via la fonction path_similarity. Ici les chats sont plus proches des chiens que des souries !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEyCe0Qmld2n",
    "outputId": "8349588e-83d5-4f66-ca41-6f7fde931e40"
   },
   "outputs": [],
   "source": [
    "mouse.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH2qzfyclw2v",
    "outputId": "9d983f8b-08d7-4734-b8f5-2be673cfcdfa"
   },
   "outputs": [],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet est une ressource pr√©cieuse pour le TAL et la RI.\n",
    "Elle permet de mieux comprendre les relations entre mots et d'am√©liorer les algorithmes de recherche et d‚Äôanalyse s√©mantique.\n",
    "Son extension √† d‚Äôautres langues facilite les applications multilingues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsXp9nhamaUd"
   },
   "source": [
    "Explorer les autres mesures de similarit√© existantes entre synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Exercise: Exploring and Analyzing Lexical Relations with WordNet (English)\n",
    "üéØ Objective:\n",
    "Understand semantic relations between words using English WordNet and analyze their usefulness in Natural Language Processing (NLP).\n",
    "\n",
    "üîπ Instructions:\n",
    "Select 3 words from different categories (a noun, a verb, and an adjective).\n",
    "\n",
    "Example: dog, run, happy\n",
    "Use the NLTK WordNet interface to:\n",
    "\n",
    "List their synonyms and antonyms (if available).\n",
    "Identify their hypernyms (more general concepts) and hyponyms (more specific concepts).\n",
    "Explore other lexical relations such as meronyms (part-whole relationships) and holonyms (whole-part relationships).\n",
    "Analyze and answer the following questions:\n",
    "\n",
    "Which word has the most lexical relations? Why?\n",
    "Do you notice ambiguities in the meanings? Provide an example.\n",
    "How could these relations improve applications like chatbots, search engines, or text analysis?\n",
    "üîπ Example Expected Answer:\n",
    "Chosen Word: \"dog\" (noun)\n",
    "\n",
    "Synonyms: Canine, domestic dog\n",
    "Hypernym: Mammal\n",
    "Hyponym: Golden Retriever, Bulldog\n",
    "Meronym: Paw, tail\n",
    "Antonym: (None found)\n",
    "Analysis:\n",
    "\n",
    "The word \"dog\" has many hyponyms, showing the variety of dog breeds.\n",
    "The verb \"run\" has multiple meanings (e.g., run a business vs. run in a race), making it ambiguous in NLP tasks.\n",
    "These relations could enhance a search engine by suggesting related terms, or help a chatbot understand user intent more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Semantic Role Labeling (SRL) Demo\n",
    "https://cogcomp.seas.upenn.edu/page/demo_view/SRLEnglish "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Charger le mod√®le de langue fran√ßais\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Cr√©er des objets doc pour les mots\n",
    "voiture = nlp(\"voiture\")\n",
    "camion = nlp(\"camion\")\n",
    "\n",
    "# Calculer la similarit√© cosinus\n",
    "similarite = voiture.similarity(camion)\n",
    "\n",
    "print(f\"Similarit√© cosinus entre 'voiture' et 'camion' : {similarite}\")\n",
    "\n",
    "# Cr√©er des objets doc pour les mots\n",
    "voiture = nlp(\"voiture\")\n",
    "camion = nlp(\"ciel\")\n",
    "\n",
    "# Calculer la similarit√© cosinus\n",
    "similarite = voiture.similarity(camion)\n",
    "\n",
    "print(f\"Similarit√© cosinus entre 'voiture' et 'ciel' : {similarite}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Word embeddings and distributional similarity\n",
    "**Exercice 1** : Analyse de la Similarit√© Distributionnelle\n",
    "üìå Objectif : Comprendre comment la similarit√© distributionnelle permet de capturer la proximit√© s√©mantique entre mots.\n",
    "\n",
    "Consignes\n",
    "On vous donne un corpus simple contenant quelques phrases. Construisez une matrice de co-occurrence (fr√©quence des mots dans une fen√™tre de contexte de taille 2).\n",
    "Calculez la similarit√© cosinus entre diff√©rents mots de votre matrice.\n",
    "Comparez les r√©sultats avec votre intuition : quels mots sont les plus proches selon cette m√©thode ?\n",
    "Donn√©es exemple\n",
    "Corpus :\n",
    "\n",
    "\"Le chat dort sur le canap√©. Le chien joue avec le chat. Le chat et le chien sont amis.\"\n",
    "\n",
    "üí° Questions de r√©flexion :\n",
    "\n",
    "Quels sont les mots les plus proches du mot \"chat\" selon votre matrice ?\n",
    "Comment la taille de la fen√™tre de contexte influence-t-elle la similarit√© ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# Petit corpus d'exemple\n",
    "corpus = [\n",
    "    [\"le\", \"chat\", \"dort\", \"sur\", \"le\", \"canap√©\"],\n",
    "    [\"le\", \"chien\", \"joue\", \"avec\", \"le\", \"chat\"],\n",
    "    [\"le\", \"chat\", \"et\", \"le\", \"chien\", \"sont\", \"amis\"]\n",
    "]\n",
    "\n",
    "# Cr√©ation du vocabulaire\n",
    "vocab = list(set(chain(*corpus)))  # Mots uniques\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "# Param√®tres\n",
    "window_size = 2\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Construction de la matrice de co-occurrence\n",
    "cooc_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for sentence in corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        word_idx = word2idx[word]\n",
    "        for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence))):\n",
    "            if i != j:\n",
    "                context_word_idx = word2idx[sentence[j]]\n",
    "                cooc_matrix[word_idx, context_word_idx] += 1\n",
    "\n",
    "# Normalisation (√©vite l'explosion des valeurs)\n",
    "cooc_matrix = cooc_matrix / torch.norm(cooc_matrix, dim=1, keepdim=True)\n",
    "\n",
    "# Fonction de similarit√© cosinus\n",
    "def cosine_similarity(word1, word2):\n",
    "    idx1, idx2 = word2idx[word1], word2idx[word2]\n",
    "    return torch.dot(cooc_matrix[idx1], cooc_matrix[idx2]).item()\n",
    "\n",
    "# Exemple de similarit√©\n",
    "print(f\"Similarit√© cosinus entre 'chat' et 'chien' : {cosine_similarity('chat', 'chien'):.4f}\")\n",
    "print(f\"Similarit√© cosinus entre 'chat' et 'canap√©' : {cosine_similarity('chat', 'canap√©'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** : Utilisation de Word2Vec et Analyse des Vecteurs\n",
    "üìå Objectif : Explorer un mod√®le de word embeddings statique et visualiser les relations entre mots.\n",
    "\n",
    "Consignes\n",
    "Chargez un mod√®le pr√©-entra√Æn√© de Word2Vec (ex. word2vec-google-news-300 via gensim).\n",
    "Trouvez les mots les plus proches (top 5) d‚Äôun mot donn√© (ex. king, dog, Paris).\n",
    "Effectuez une op√©ration vectorielle s√©mantique : testez l‚Äôanalogie king - man + woman = ?\n",
    "Visualisez les embeddings de quelques mots avec t-SNE ou PCA.\n",
    "üí° Questions de r√©flexion :\n",
    "\n",
    "Les mots les plus proches sont-ils ceux auxquels vous vous attendiez ?\n",
    "Pourquoi l‚Äôanalogie king - man + woman donne-t-elle queen ?\n",
    "Quels sont les avantages et limites des embeddings statiques ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)  # V√©rifie que la biblioth√®que est bien install√©e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Charger le mod√®le Word2Vec pr√©-entra√Æn√© (Google News, 300 dimensions)\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Conversion des embeddings en tensors PyTorch\n",
    "def get_embedding(word):\n",
    "    return torch.tensor(word_vectors[word]) if word in word_vectors else None\n",
    "\n",
    "# Calcul de la similarit√© cosinus entre deux mots\n",
    "def word_similarity(word1, word2):\n",
    "    vec1, vec2 = get_embedding(word1), get_embedding(word2)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        return F.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "    return None\n",
    "\n",
    "# Test de similarit√©\n",
    "print(f\"Similarit√© entre 'king' et 'queen' : {word_similarity('king', 'queen'):.4f}\")\n",
    "print(f\"Similarit√© entre 'dog' et 'cat' : {word_similarity('dog', 'cat'):.4f}\")\n",
    "\n",
    "# Analogie : king - man + woman ‚âà ?\n",
    "def analogy(word1, word2, word3):\n",
    "    vec1, vec2, vec3 = get_embedding(word1), get_embedding(word2), get_embedding(word3)\n",
    "    if None not in [vec1, vec2, vec3]:\n",
    "        result_vector = vec1 - vec2 + vec3\n",
    "        similar_words = word_vectors.similar_by_vector(result_vector.numpy(), topn=1)\n",
    "        return similar_words[0][0]\n",
    "    return None\n",
    "\n",
    "print(f\"'king' - 'man' + 'woman' ‚âà {analogy('king', 'man', 'woman')}\")\n",
    "\n",
    "# üìä **Visualisation des embeddings avec t-SNE**\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"dog\", \"cat\", \"paris\", \"france\"]\n",
    "embeddings = torch.stack([get_embedding(w) for w in words if get_embedding(w) is not None])\n",
    "\n",
    "# R√©duction de dimension avec t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings.numpy())\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in enumerate(words):\n",
    "    if i < len(embeddings_2d):  # Ignore words non pr√©sents\n",
    "        x, y = embeddings_2d[i]\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.02, y + 0.02, word, fontsize=12)\n",
    "\n",
    "plt.title(\"Visualisation des Word Embeddings avec t-SNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Charger le mod√®le pr√©-entra√Æn√© Word2Vec (Google News)\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Trouver les mots les plus proches de \"chien\"\n",
    "similar_words = word_vectors.most_similar(\"dog\", topn=5)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez le vecteur de chaque mot.\n",
    "Calculez la similarit√© entre ces vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Exercice : Analyse des Word Embeddings sur un Dataset Wikipedia\n",
    "üìå Objectif : Explorer les word embeddings en utilisant un dataset extrait de Wikipedia.\n",
    "\n",
    "√âtapes √† suivre\n",
    "1Ô∏è‚É£ Charger et pr√©traiter le dataset üìÇ\n",
    "\n",
    "Utiliser un corpus extrait de Wikipedia.\n",
    "Nettoyer le texte (suppression des stopwords, ponctuation, etc.).\n",
    "2Ô∏è‚É£ Analyse des fr√©quences üìä\n",
    "\n",
    "Identifier les 5 mots les plus fr√©quents dans le corpus.\n",
    "3Ô∏è‚É£ Exploration des similarit√©s üîç\n",
    "\n",
    "Pour chacun de ces mots, trouver les 5 mots les plus proches en utilisant les embeddings.\n",
    "4Ô∏è‚É£ Visualisation des embeddings üé®\n",
    "\n",
    "Repr√©senter les embeddings en 2D avec t-SNE.\n",
    "Comparer cette visualisation avec une r√©duction de dimension PCA.\n",
    "üí° Question de r√©flexion : Quels sont les avantages et les limites de t-SNE et PCA pour visualiser les embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projector: http://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
